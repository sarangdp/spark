package exercise

import com.typesafe.config.ConfigFactory
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.functions._


case class Orders(
      order_id:Int,
      order_date:String,
      order_customer_id:Int,
      order_status:String)
case class OrderItems(order_item_id: Int,
      order_item_order_id: Int,
      order_item_product_id: Int,
      order_item_quantity: Int,
      order_item_subtotal: Float,
      order_item_price: Float)
case class Products(product_id: Int,
      product_category_id: Int,
      product_name: String,
      product_description: String,
      product_price: Float,
       product_image: String)
case class Categories(category_id: Int,
       category_department_id: Int,
       category_name: String)
case class Departments(department_id: Int,
       department_name: String)

object RevPerDeptWithDFSQL {
    def main (args : Array[String]): Unit ={
      val props = ConfigFactory.load()
      val conf = new SparkConf().
        setAppName("Rev Per Dept With DF SQL")
        .setMaster(props.getConfig(args(0)).getString("executionMode"))
      val sc = new SparkContext(conf)
      val sqlContext = new SQLContext(sc)
      import sqlContext.implicits._

      val inputPath = args(1)
      val outputPath = args(2)

      val fs :FileSystem = FileSystem.get(sc.hadoopConfiguration)
      val op = new Path(outputPath)
      if(!fs.exists(new Path(inputPath))){
        println("Invalid input path")
      }

      if(fs.exists(op)){
        fs.delete(op, true)
      }

      val ordersDF = sc.textFile(inputPath + "/orders").map(rec => {
        val r = rec.split(",")
        Orders(r(0).toInt, r(1), r(2).toInt, r(3))
      }).toDF()

      val orderItemsDF = sc.textFile(inputPath+"/order_items").
        map(rec => {
          val a = rec.split(",")
          OrderItems(
            a(0).toInt,
            a(1).toInt,
            a(2).toInt,
            a(3).toInt,
            a(4).toFloat,
            a(5).toFloat)
        }).toDF()

      val productsDF = sc.textFile(inputPath+"/products").filter(rec => !rec.startsWith("685")).map(rec => {
        val r = rec.split(",")
        Products(r(0).toInt, r(1).toInt, r(2), r(3), r(4).toFloat, r(5))
      }).toDF()

      val categoriesDF = sc.textFile(inputPath+"/categories").map(rec => {
        val r = rec.split(",")
        Categories(r(0).toInt, r(1).toInt, r(2))
      }).toDF()

      val deptDF = sc.textFile(inputPath+"/departments").map(rec => {
        val r = rec.split(",")
        Departments(r(0).toInt, r(1))
      }).toDF()

      ordersDF.registerTempTable("orders")
      orderItemsDF.registerTempTable("orderItems")
      productsDF.registerTempTable("products")
      categoriesDF.registerTempTable("categories")
      deptDF.registerTempTable("dept")

      sqlContext.sql("select d.department_name,sum(oi.order_item_subtotal) total_rev from orders o join orderItems oi on o.order_id = oi.order_item_order_id "+
                " join products p on oi.order_item_product_id = p.product_id join categories c on p.product_category_id = c.category_id join dept d on "+
                " c.category_department_id = d.department_id group by d.department_name").rdd.saveAsTextFile(outputPath)
    }
}
